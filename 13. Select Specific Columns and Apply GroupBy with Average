//scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._


val data = Seq(
    ("Laptop", "Electronics", 1000, 2),
    ("Phone", "Electronics", 500, 1),
    ("T-Shirt", "Clothing", 20, 3),
    ("Jeans", "Clothing", 50, 4)
).toDF("product", "category" , "price" , "quantity")


val groupByProduct = data.select("product","price").groupBy("product").agg(avg("price").alias("average_price"))

groupByProduct.show()


//pyspark


from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    ("Laptop", "Electronics", 1000, 2),
    ("Phone", "Electronics", 500, 1),
    ("T-Shirt", "Clothing", 20, 3),
    ("Jeans", "Clothing", 50, 4)
]

df = spark.createDataFrame(data, ["product", "category" , "price" , "quantity"])


groupByProduct = df.select("product","price").groupBy("product").agg(avg("price").alias("average_price"))

groupByProduct.show()
