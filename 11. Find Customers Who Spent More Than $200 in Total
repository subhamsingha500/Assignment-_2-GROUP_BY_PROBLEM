//scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._


val data = Seq(
    (1, "order_1", 100),
    (1, "order_2", 150),
    (2, "order_3", 250),
    (3, "order_4", 100),
    (3, "order_5", 120)
).toDF("customer_id", "order_id","amount")


val groupBySpend = data.groupBy("customer_id").agg(sum("amount").alias("total_spend")).filter(col("total_spend")>200)


groupBySpend.show()

//pyspark

%python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    (1, "order_1", 100),
    (1, "order_2", 150),
    (2, "order_3", 250),
    (3, "order_4", 100),
    (3, "order_5", 120)
]

df = spark.createDataFrame(data, ["customer_id", "order_id","amount"])


groupBySpend = df \
    .groupBy("customer_id") \
    .agg(sum("amount").alias("total_spend")) \
    .filter(col("total_spend")>200) 

groupBySpend.show()
