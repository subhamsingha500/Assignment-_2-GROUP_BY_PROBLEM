//scala 

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._

val data = Seq(
    (1, 1, 4),
    (1, 2, 5),
    (2, 1, 3),
    (2, 3, 4),
    (3, 2, 5)
).toDF("user_id", "product_id", "rating")


val groupByRatingAvg = data.groupBy("user_id").agg(avg("rating").alias("average_rating"))

groupByRatingAvg.show()

//pyspark


from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    (1, 1, 4),
    (1, 2, 5),
    (2, 1, 3),
    (2, 3, 4),
    (3, 2, 5)
]

df = spark.createDataFrame(data, ["user_id", "product_id", "rating"])

groupByRatingAvg = df.groupBy("user_id").agg(avg("rating").alias("average_rating"))

groupByRatingAvg.show()
