//scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._


val data = Seq(
    ("2023-01-01", 1, 100),
    ("2023-02-15", 2, 200),
    ("2023-03-10", 3, 300),
    ("2023-04-20", 1, 400),
    ("2023-05-05", 2, 500)
).toDF("order_date", "customer_id" , "amount")


val groupByDate = data.groupBy(month(col("order_date")),year(col("order_date"))).agg(count("customer_id").alias("total_orders"))
val groupByAmount = data.groupBy(month(col("order_date")),year(col("order_date"))).agg(sum("amount").alias("total_amount"))

groupByDate.show()
groupByAmount.show()

//Pyspark

%python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    ("2023-01-01", 1, 100),
    ("2023-02-15", 2, 200),
    ("2023-03-10", 3, 300),
    ("2023-04-20", 1, 400),
    ("2023-05-05", 2, 500)
]

df = spark.createDataFrame(data, ["order_date", "customer_id" , "amount"])


groupByDate = df\
    .groupBy(month(col("order_date")),year(col("order_date")))\
    .agg(count("customer_id").alias("total_orders"))

groupByAmount = df\
    .groupBy(month(col("order_date")),year(col("order_date")))\
    .agg(sum("amount").alias("total_amount"))


groupByDate.show()
groupByAmount.show()
