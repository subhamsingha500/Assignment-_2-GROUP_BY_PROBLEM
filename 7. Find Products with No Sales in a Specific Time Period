//scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._

val data = Seq(
    ("Laptop", "2023-01-01"),
    ("Phone", "2023-02-15"),
    ("T-Shirt", "2023-03-10"),
    ("Jeans", "2023-04-20")
).toDF("product", "order_date")


val groupByProduct = data.groupBy("product")
    .agg(max(when(col("order_date").between("2023-02-01", "2023-03-31"), 1).otherwise(0)).alias("has_sales"))
    .filter(col("has_sales") === 0)
    .select("product")


groupByProduct.show()

//pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    ("Laptop", "2023-01-01"),
    ("Phone", "2023-02-15"),
    ("T-Shirt", "2023-03-10"),
    ("Jeans", "2023-04-20")
]

df = spark.createDataFrame(data, ["product", "order_date"])

groupByProduct = df.groupBy("product") \
    .agg(max(when(col("order_date").between("2023-02-01", "2023-03-31"), 1).otherwise(0)).alias("has_sales")) \
    .filter(col("has_sales") == 0) \
    .select("product")


groupByProduct.show()
