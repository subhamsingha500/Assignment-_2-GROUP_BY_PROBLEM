//scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._

val data = Seq(
    ("2023-01-01", "New York", 100),
    ("2023-02-15", "London", 200),
    ("2023-03-10", "Paris", 300),
    ("2023-04-20", "Berlin", 400),
    ("2023-05-05", "Tokyo", 500)
).toDF("order_date", "city", "amount")


val groupByMonth = data.groupBy(month(col("order_date"))).agg(sum("amount"))

val groupByYear = data.groupBy(year(col("order_date"))).agg(sum("amount"))

val groupBydate = data.groupBy("order_date").agg(sum("amount"))



groupByMonth.show()
groupByYear.show()
groupBydate.show()

//pyspark

%python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    ("2023-01-01", "New York", 100),
    ("2023-02-15", "London", 200),
    ("2023-03-10", "Paris", 300),
    ("2023-04-20", "Berlin", 400),
    ("2023-05-05", "Tokyo", 500)
]

df = spark.createDataFrame(data, ["order_date", "city", "amount"])

groupByMonth = df.groupBy(month(col("order_date"))).agg(sum("amount"))

groupByYear = df.groupBy(year(col("order_date"))).agg(sum("amount")) 

groupBydate = df.groupBy("order_date").agg(sum("amount"))

groupByMonth.show()
groupByYear.show()
groupBydate.show()
