//scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._

val data = Seq(
    (1, "USA", "order_1", 100),
    (1, "USA", "order_2", 200),
    (2, "UK", "order_3", 150),
    (3, "France", "order_4", 250),
    (3, "France", "order_5", 300)
).toDF("customer_id", "country", "order_id","amount")

val groupByCountry = data.groupBy("country").agg(sum("amount").alias("total_spend"))

groupByCountry.show()


//pyspark

%python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    (1, "USA", "order_1", 100),
    (1, "USA", "order_2", 200),
    (2, "UK", "order_3", 150),
    (3, "France", "order_4", 250),
    (3, "France", "order_5", 300)
]

df = spark.createDataFrame(data, ["customer_id", "country", "order_id","amount"])

groupByCountry = df.groupBy("country").agg(sum("amount").alias("total_spend"))



groupByCountry.show()
