//scala

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

val spark = SparkSession.builder
  .appName("Group_By2")
  .getOrCreate()

import spark.implicits._


val data = Seq(
    ("T-Shirt", "Clothing", 20),
    ("Table", "Furniture", 150),
    ("Jeans", "Clothing", 50),
    ("Chair", "Furniture", 100)
).toDF("product", "category","price")


val groupByProduct = data.filter(col("product").startsWith("T")).groupBy("category").agg(avg("price"))


groupByProduct.show()

//pyspark

%python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("GroupBy").getOrCreate()

data = [
    ("T-Shirt", "Clothing", 20),
    ("Table", "Furniture", 150),
    ("Jeans", "Clothing", 50),
    ("Chair", "Furniture", 100)
]

df = spark.createDataFrame(data, ["product", "category","price"])

groupByProduct = df \
    .filter(col("product").startswith("T")) \
    .groupBy("category") \
    .agg(avg("price").alias("average_price"))

groupByProduct.show()

